---
title: "Regularization"
author: "Sneiba"
date: "12/6/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
### Model Complexity

![optional caption text](screenshots/model_complexity.png)

### Maximum Likelihood Estimator

The maximum likelihood estimator is a technique for estimating model parameters. It answers the question: "Which parameters will most likely to characterize the dataset"


Traditional statistics has concentrated on the case of $d \ll n$ (the number of parameters to be estimated is
much smaller than the train set size). In that case the maximum likelihood estimator (MLE) performs well
and its asymptotic optimality 'kicks in' (recall the asymptotic variance of the MLE is the best possible -
the inverse Fisher information). However, as higher and higher dimensional parameter spaces are considered
it was discovered that the MLE performs poorly. It overfits the training data and performs poorly on future
test data. In other words, when $d \gg n$ the MLE picks up noise as signal, for example by assuming that
irrelevant features are relevant due to small train set size.

### Mean Squared Error


![](screenshots/mse.png)
### Regularization Methods

Here are some regularization methods :

- James-Stein Shrinkage
- Breiman's Garrote
- Ridge Estimator
- Lasso Estimator
- Elastic Net Estimator

All of these methods are meant to handle the cases where the traditional case in statistics where $d$ is much lower than $n$ does not hold. So we may have overfitting, and the regular $MLE$ may not be the right way to proceed.

#### James-Stein Shrinkage


```{r}

bball = read.table("http://www.swarthmore.edu/NatSci/peverso1/Sports%20Data/JamesSteinData/Efron-Morris%20Baseball/EfronMorrisBB.txt",
                   header=TRUE, stringsAsFactors=FALSE)
bball$js = bball$BattingAverage * .212 + .788 * (0.265)
bball$LastName[!is.na(match(bball$LastName, 
  c("Scott","Williams", "Rodriguez", "Unser","Swaboda","Spencer")))] = ""

a = matrix(rep(1:3, nrow(bball)), 3, nrow(bball))
b = matrix(c(bball$BattingAverage, bball$SeasonAverage, bball$js), 
   3, nrow(bball), byrow=TRUE)
matplot(a, b, pch=" ", ylab="predicted average", xaxt="n", xlim=c(0.5, 3.1), ylim=c(0.13, 0.42))
matlines(a, b)
text(rep(0.7, nrow(bball)), bball$BattingAverage, bball$LastName, cex=0.6)
text(1, 0.14, "First 45\nat bats", cex=0.5)
text(2, 0.14, "Average\nof remainder", cex=0.5)
text(3, 0.14, "J-S\nestimator", cex=0.5)

```
